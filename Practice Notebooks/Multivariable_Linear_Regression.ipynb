{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Multivariable-Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as ply\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2\"></a>\n",
    "\n",
    "You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  \n",
    "\n",
    "Please run the following code cell to create your `X_train` and `y_train` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "X_train=np.array([[2104,5,1,45],  # Training Set Number -1\n",
    "                 [1416,3,2,40], # Training set Number -2 \n",
    "                 [852,2,1,35]])  # Training set Number -3\n",
    " \n",
    "Y_train=np.array([460,232,178])\n",
    "#  We have to treat the Whole Operation Like Two-Dimensional Array i and j \n",
    "print(X_train[1,2]) # Operations of 2-Dimensional Arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the compute_function \n",
    "def compute_function(x,w,b):\n",
    "    f_wb=np.dot(x,w) +b\n",
    "    return f_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that here the parameter `x` and `w `passed represents Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the compute_cost\n",
    "def compute_cost(x,y,w,b):\n",
    "    m=x.shape[0]\n",
    "    cost=0.0\n",
    "    for i in range(m):\n",
    "        temp_i=(np.dot(x[i],w)-y[i])**2\n",
    "        cost += temp_i\n",
    "    cost = cost/(2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Point (Added whole code for all three functions to the notebook) :\n",
    "for j in range(n):                         \n",
    "            `dj_dw[j] = dj_dw[j] + err * X[i, j]`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calculating the Derivatives Vector \n",
    "def gradient_cal(x,y,w,b):\n",
    "# df_dw --> Will be a vector \n",
    "# df_db --> will be a list\n",
    "\n",
    "# X_train= [[2104,5,1,45](i=0),[1416,3,2,40](i=1), [852,2,1,35](i=2)]\n",
    "# m=3\n",
    "# n=4\n",
    "# w = [w1 w2 w3 w4]\n",
    "# y = [y1 y2 y3 ]\n",
    "\n",
    "\n",
    "    df_db=0\n",
    "    n=x.shape[1]\n",
    "    m=x.shape[0]\n",
    "    df_dw=np.zeros(n)   # Initializing the List with required size to avoid the error\n",
    "    for i in range(m):\n",
    "        common_part = (compute_function(x[i],w,b) -y[i])\n",
    "        for j in range(n):\n",
    "            df_dw[j]+=common_part*x[i,j]  # This denotes an Element not any other thing\n",
    "        df_db+=common_part\n",
    "    df_dw=df_dw/m\n",
    "    df_db=df_db/m\n",
    "\n",
    "    return df_dw,df_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Gradient Descent Algorithm\n",
    "def descent_algo(x,y,learning_rate,iterations):\n",
    "\n",
    "    m=x.shape[0]\n",
    "    n=x.shape[1]\n",
    "    w_init = np.random.rand(4)\n",
    "    b_init=0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        temp_w=w_init-learning_rate*gradient_cal(x,y,w_init,b_init)[0]\n",
    "        temp_b=b_init-learning_rate*gradient_cal(x,y,w_init,b_init)[1]\n",
    "        w_init=temp_w # Here we are updating a list and not any single variable \n",
    "        b_init=b_init\n",
    "        \n",
    "    return w_init,b_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Values using the gradient Descent Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sian\\AppData\\Local\\Temp\\ipykernel_11572\\1847686380.py:10: RuntimeWarning: invalid value encountered in subtract\n",
      "  temp_w=w_init-learning_rate*gradient_cal(x,y,w_init,b_init)[0]\n",
      "C:\\Users\\Sian\\AppData\\Local\\Temp\\ipykernel_11572\\4292002400.py:20: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  df_dw[j]+=common_part*x[i,j]  # This denotes an Element not any other thing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Optimized Values for W(weight): [nan nan nan nan] The Optimised Value for b(bias) : 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Optimized Values for W(weight): {descent_algo(X_train,Y_train,0.1,10000)[0]} The Optimised Value for b(bias) : {descent_algo(X_train,Y_train,0.1,10000)[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
